import os
import numpy as np
from ultralytics import YOLO
import time
import multiprocessing
from pathlib import Path
from PIL import Image
from scipy.stats import entropy


def load_image_paths(directory):
    return [str(path) for path in Path(directory).rglob('*') if
            path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']]


# Paths to split directories
train_dir = r"C:\Users\xxxxx\cocodataset\images\train"
val_dir = r"C:\Users\xxxxx\cocodataset\images\val"
test_dir = r"C:\Users\xxxxx\cocodataset\images\test"

# Load image paths for each split
train_images = load_image_paths(train_dir)
val_images = load_image_paths(val_dir)
test_images = load_image_paths(test_dir)

print(f"Train set size: {len(train_images)}")
print(f"Validation set size: {len(val_images)}")
print(f"Test set size: {len(test_images)}")

CLASSES_OF_INTEREST = ["traffic light", "fire hydrant", "stop sign", "bird", "cat"]


def calculate_iou(box1, box2):
    # Convert [x_center, y_center, width, height] to [x_min, y_min, x_max, y_max]
    box1 = [box1[0] - box1[2] / 2, box1[1] - box1[3] / 2, box1[0] + box1[2] / 2, box1[1] + box1[3] / 2]
    box2 = [box2[0] - box2[2] / 2, box2[1] - box2[3] / 2, box2[0] + box2[2] / 2, box2[1] + box2[3] / 2]

    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    iou = intersection / float(box1_area + box2_area - intersection)
    return iou


def calculate_metrics(predictions, ground_truth, iou_thresholds=np.arange(0.5, 1.0, 0.05)):
    aps = []
    true_positives = np.zeros(len(predictions))
    false_positives = np.zeros(len(predictions))

    for i, pred in enumerate(predictions):
        best_iou = 0
        best_gt = None
        for gt in ground_truth:
            if pred['label'] == gt['label']:
                iou = calculate_iou(pred['bounding_box'], gt['bounding_box'])
                if iou > best_iou:
                    best_iou = iou
                    best_gt = gt

        if best_iou >= 0.5:  # Use 0.5 IoU threshold for precision, recall, and F1
            if not best_gt.get('matched', False):
                true_positives[i] = 1
                best_gt['matched'] = True
            else:
                false_positives[i] = 1
        else:
            false_positives[i] = 1

    # Compute precision, recall, and F1 score
    tp_sum = np.sum(true_positives)
    fp_sum = np.sum(false_positives)
    fn_sum = len(ground_truth) - tp_sum

    precision = tp_sum / (tp_sum + fp_sum) if (tp_sum + fp_sum) > 0 else 0
    recall = tp_sum / (tp_sum + fn_sum) if (tp_sum + fn_sum) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    # Compute mAP
    for iou_threshold in iou_thresholds:
        ap = calculate_ap_at_iou(predictions, ground_truth, iou_threshold)
        aps.append(ap)

    mAP50 = aps[0]
    mAP50_95 = np.mean(aps)

    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'mAP50': mAP50,
        'mAP50-95': mAP50_95
    }


def calculate_ap(recalls, precisions):
    # Add sentinel values to beginning and end
    recalls = np.concatenate(([0.], recalls, [1.]))
    precisions = np.concatenate(([0.], precisions, [0.]))

    # Compute the precision envelope
    for i in range(precisions.size - 1, 0, -1):
        precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])

    # Compute area under PR curve
    indices = np.where(recalls[1:] != recalls[:-1])[0]
    ap = np.sum((recalls[indices + 1] - recalls[indices]) * precisions[indices + 1])
    return ap


def calculate_ap_at_iou(predictions, ground_truth, iou_threshold):
    true_positives = np.zeros(len(predictions))
    false_positives = np.zeros(len(predictions))

    for i, pred in enumerate(predictions):
        best_iou = 0
        for gt in ground_truth:
            if pred['label'] == gt['label']:
                iou = calculate_iou(pred['bounding_box'], gt['bounding_box'])
                if iou > best_iou:
                    best_iou = iou

        if best_iou >= iou_threshold:
            true_positives[i] = 1
        else:
            false_positives[i] = 1

    # Compute precision and recall
    cumsum = np.cumsum(true_positives)
    cumsum_fp = np.cumsum(false_positives)
    recalls = cumsum / len(ground_truth)
    precisions = cumsum / (cumsum + cumsum_fp)

    return calculate_ap(recalls, precisions)


def calculate_uncertainty(probabilities):
    return entropy(probabilities)


def main():
    # Load YOLOv8x model
    model = YOLO("yolov8x.pt")

    # Train the model
    print("Training the model...")
     results = model.train(
        data="C:/Users/lenam/PycharmProjects/NLDL/custom_dataset.yaml",  # Path to the dataset YAML file
        epochs=100,
        imgsz=640,
        batch=16,
        name='yolov8x_custom'
     )

    # Load the best trained model
    best_model = YOLO("runs/detect/yolov8x_custom10/weights/best.pt")

    # Evaluating the model on the test set
    all_metrics = []
    uncertainty_metrics = {class_name: [] for class_name in CLASSES_OF_INTEREST}
    size_uncertainty_correlation = []
    accuracy_uncertainty_correlation = []
    total_samples = len(test_images)
    start_time = time.time()

    for i, image_path in enumerate(test_images, 1):
        # Get image dimensions
        with Image.open(image_path) as img:
            image_width, image_height = img.size

        # Run YOLOv8 inference with the best trained model
        results = best_model(image_path, verbose=False)

        # Convert YOLOv8 results to detections, filtering for classes of interest
        detections = []
        for r in results:
            boxes = r.boxes.xywhn.cpu().numpy()  # Use normalized coordinates
            confs = r.boxes.conf.cpu().numpy()
            cls = r.boxes.cls.cpu().numpy()

            for box, conf, cl in zip(boxes, confs, cls):
                label = best_model.names[int(cl)]
                if label in CLASSES_OF_INTEREST:
                    detection = {
                        'label': label,
                        'bounding_box': box.tolist(),
                        'confidence': float(conf),
                    }
                    detections.append(detection)

                    # Use 1 - confidence as a proxy for uncertainty
                    uncertainty = 1 - float(conf)
                    uncertainty_metrics[label].append(uncertainty)

                    # Correlation between object size and uncertainty
                    object_size = box[2] * box[3]  # width * height
                    size_uncertainty_correlation.append((object_size, uncertainty))

        # Get ground truth
        label_path = image_path.replace('images', 'labels').rsplit('.', 1)[0] + '.txt'
        ground_truth = []
        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:  # class x y w h
                        class_idx = int(parts[0])
                        class_name = best_model.names[class_idx]
                        if class_name in CLASSES_OF_INTEREST:
                            ground_truth.append({
                                'label': class_name,
                                'bounding_box': [float(coord) for coord in parts[1:]]  # Already normalized
                            })

        # Calculate metrics
        metrics = calculate_metrics(detections, ground_truth)
        all_metrics.append(metrics)

        # Correlation between detection accuracy and uncertainty
        for det in detections:
            best_iou = 0
            for gt in ground_truth:
                if det['label'] == gt['label']:
                    iou = calculate_iou(det['bounding_box'], gt['bounding_box'])
                    best_iou = max(best_iou, iou)
            uncertainty = 1 - det['confidence']
            accuracy_uncertainty_correlation.append((best_iou, uncertainty))

        # Print progress
        print(f"\rProcessed {i}/{total_samples} images", end="", flush=True)

    end_time = time.time()
    processing_time = end_time - start_time
    print(f"\nAll images processed in {processing_time:.2f} seconds.")

    # Calculate average metrics
    avg_metrics = {
        'precision': np.mean([m['precision'] for m in all_metrics]),
        'recall': np.mean([m['recall'] for m in all_metrics]),
        'f1_score': np.mean([m['f1_score'] for m in all_metrics]),
        'mAP50': np.mean([m['mAP50'] for m in all_metrics]),
        'mAP50-95': np.mean([m['mAP50-95'] for m in all_metrics])
    }

    print("\nValidation Set Metrics:")
    print("Average Metrics:", avg_metrics)

    # Print uncertainty metrics
    print("\nUncertainty Metrics:")
    for class_name, uncertainties in uncertainty_metrics.items():
        if uncertainties:
            print(
                f"{class_name}: Mean Uncertainty = {np.mean(uncertainties):.4f}, Std Uncertainty = {np.std(uncertainties):.4f}")

    # Calculate correlations
    size_uncertainty_corr = np.corrcoef(np.array(size_uncertainty_correlation).T)[0, 1]
    accuracy_uncertainty_corr = np.corrcoef(np.array(accuracy_uncertainty_correlation).T)[0, 1]

    print("\nCorrelations:")
    print(f"Object Size vs Uncertainty: {size_uncertainty_corr:.4f}")
    print(f"Detection Accuracy vs Uncertainty: {accuracy_uncertainty_corr:.4f}")


if __name__ == '__main__':
    multiprocessing.freeze_support()  # This is necessary for Windows
    main()
